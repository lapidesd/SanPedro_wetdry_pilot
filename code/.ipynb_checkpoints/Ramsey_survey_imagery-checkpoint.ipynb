{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf27ff6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m \n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m timedelta\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshapely\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Point\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv \n",
    "import geopandas as gpd\n",
    "from datetime import timedelta\n",
    "from shapely.geometry import Point\n",
    "import glob\n",
    "from shapely import wkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614892c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv files and adjust to datetime\n",
    "R_im_date = pd.read_csv('../data/Ramsey/RamseyImageryDates.csv', parse_dates=['date'])\n",
    "R_im_date['date'] = pd.to_datetime(R_im_date['date'])\n",
    "\n",
    "#years = range(2015, 2025)\n",
    "#pd.DataFrame([pd.Timestamp(year=year, month=6, day=15) for year in years], columns = ['date'])\n",
    "R_sur_date = pd.read_csv('../data/Ramsey/RamseySurveyDates.csv', delimiter=';', index_col=False, parse_dates = ['Ramsey wet/dry date']) \n",
    "R_sur_date.rename(columns={'Ramsey wet/dry date':'date'}, inplace=True )\n",
    "R_sur_date['date'] = pd.to_datetime(R_sur_date['date'], format = '%m/%d/%Y', errors = 'coerce')\n",
    "R_sur_date = R_sur_date.dropna(subset = ['date'])\n",
    "\n",
    "R_hyd = pd.read_csv('../data/Ramsey/RamseyHydroData.csv')\n",
    "R_hyd.rename( columns={'Unnamed: 0':'date'}, inplace=True )\n",
    "R_hyd['date'] = pd.to_datetime(R_hyd['date'])\n",
    "\n",
    "R_precipitation = pd.read_csv('../data/Ramsey/daymet_precip.csv')\n",
    "R_precipitation['system:time_start'] = pd.to_datetime(R_precipitation['system:time_start'])\n",
    "R_precipitation.rename( columns={'00000000000000000000':'P','system:time_start':'day'}, inplace=True )\n",
    "\n",
    "R_surveyData = pd.read_csv('../data/Ramsey/Ramsey_surveyData.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6579e35-92e1-4944-a028-1b26cd64170e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136bd05c-7e26-40ee-9060-31aae45735d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find matching dates between survey and imagery\n",
    "matching_dates = []\n",
    "tolerance = timedelta(days = 5)\n",
    "\n",
    "for date1 in R_sur_date['date']:\n",
    "    exact_date = False\n",
    "    tol = False \n",
    "    for date2 in R_im_date['date']:\n",
    "        if date1 == date2:\n",
    "            matching_dates.append({'Survey': date1, 'Imagery': date2})\n",
    "            exact_date = True\n",
    "    if not exact_date:\n",
    "        for date2 in R_im_date['date']:\n",
    "            if abs(date1 - date2) <= tolerance:\n",
    "                matching_dates.append({'Survey': date1, 'Imagery': date2})\n",
    "                tol = True\n",
    "        if not tol: \n",
    "            for date2 in R_im_date['date']:\n",
    "                if abs(date1-date2) < timedelta(days = 10): \n",
    "                    matching_dates.append({'Survey': date1, 'Imagery': date2})\n",
    "\n",
    "\n",
    "matching_df = pd.DataFrame(matching_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af075f-4a00-44e5-a7ec-3ffc77f08284",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_datessurData = pd.merge(matching_df, R_hyd, left_on = 'Survey', right_on = 'date', how = 'left')\n",
    "R_datesimData = pd.merge(matching_df, R_hyd, left_on = 'Imagery', right_on = 'date')\n",
    "#R_datessurData = R_datessurData.drop(columns = ['Imagery','date'])\n",
    "#R_datesimData = R_datesimData.drop(columns = ['Survey','date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be3bf09-be22-48d4-9e7e-33431c0e6bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum precipitation for dates in between survey and imagery\n",
    "def sum_pdatesbetween(d1, d2):\n",
    "    r = pd.date_range(start=min(d1,d2), end=max(d1,d2))\n",
    "    return R_hyd[R_hyd['date'].isin(r)]['P [mm]'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f61845b-751d-4bd5-b478-a4bb129d35d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a dataframe to determine which imagery dates to use\n",
    "Rh = pd.DataFrame([])\n",
    "\n",
    "Rh['Survey'] = matching_df['Survey']\n",
    "Rh['Imagery'] = matching_df['Imagery']\n",
    "Rh['sum_P'] = [sum_pdatesbetween(R_datessurData.loc[i, 'Survey'], R_datesimData.loc[i, 'Imagery']) for i in range(len(Rh))]\n",
    "Rh['Q_s-i'] = (R_datessurData['Q [mm/d]'] - R_datesimData['Q [mm/d]']) / R_datessurData['Q [mm/d]'] * 100\n",
    "Rh['Use/not'] = ['use', 'use', 'use', 'use', 'not', 'not',\n",
    "                 'not', 'use', 'not', 'use'] \n",
    "\n",
    "Rh = Rh.fillna('')\n",
    "\n",
    "conditions = (Rh['Use/not'] == 'not')\n",
    "\n",
    "Rh = Rh[~conditions]\n",
    "\n",
    "Rh = Rh.drop(columns=['Use/not'])\n",
    "\n",
    "Rh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292f79bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rh.to_csv('../data/Ramsey/Ramsey_survey_imagery_hydro.csv', index = 'False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e5bcfb-ab6b-467e-adf1-ae48d180b738",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging a data frame to match the survey data to imagery dates\n",
    "#First adjusting the dates in surveyData, only logged with the respective year and not exact date\n",
    "R_sur_date['Year'] = R_sur_date['date'].dt.year\n",
    "R_surveyData['Year'] = R_surveyData['Year'].astype(int)\n",
    "Rdata = pd.merge(R_surveyData, R_sur_date[['date', 'Year']], on = 'Year', how = 'left')\n",
    "Rdata = Rdata.drop(columns=['Year', 'Unnamed: 0'])\n",
    "Rdata.rename(columns = {'date':'Year'}, inplace = True)\n",
    "Rdata = Rdata.drop_duplicates()\n",
    "Rdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44f0dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the survey data with imagery dates into a geodataframe and adding x and y from the geometry to \n",
    "#facilitate merge\n",
    "\n",
    "Rdata['geometry'] = Rdata['geometry'].apply(wkt.loads)\n",
    "gdf = gpd.GeoDataFrame(Rdata, geometry = 'geometry', crs='EPSG:26912')\n",
    "\n",
    "gdf['x'] = gdf.geometry.x\n",
    "gdf['y'] = gdf.geometry.y\n",
    "gdf = gdf[['geometry', 'x', 'y', 'wetdry', 'Year']]\n",
    "#gdf['Year'] = gdf['Year'].astype(int) \n",
    "#gdf['Year'] = pd.to_datetime(gdf['Year'])\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896019d-4589-4859-8401-f1094a58e36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for perennial reaches by comparing surveys of each year\n",
    "perennial = pd.DataFrame(gdf.groupby('geometry')['wetdry'].apply(lambda x: sum(x == 'wet'))).reset_index(drop=False)\n",
    "\n",
    "# whichever number is reasonable based on data?\n",
    "perennial = perennial[(perennial['wetdry'] == 6)]\n",
    "\n",
    "#assume always wet\n",
    "perennial = perennial.assign(wetdry = 'wet')\n",
    "\n",
    "#perennial['geometry'] = perennial['geometry'].apply(wkt.loads)\n",
    "gdf_perennial = gpd.GeoDataFrame(perennial, geometry = 'geometry', crs='EPSG:26912')\n",
    "gdf_perennial['x'] = gdf_perennial.geometry.x\n",
    "gdf_perennial['y'] = gdf_perennial.geometry.y\n",
    "\n",
    "#making the gdf matching the perennial reaches to the imagery dates available \n",
    "imagery_perennial = pd.concat([gdf_perennial.assign(imagery = date) for date in R_im_date['date']], ignore_index=True)\n",
    "imagery_perennial = imagery_perennial[~imagery_perennial['imagery'].isin(Rh['Imagery'])]\n",
    "imagery_perennial['assumption'] = len(imagery_perennial)*['assumed perennial']\n",
    "imagery_perennial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bda928-6b23-4b59-80b9-03f33de4cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_gdf = gdf[gdf['wetdry'] == 'wet']\n",
    "filtered_gdf.groupby('geometry').count().wetdry.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2623e5ff-7121-449f-bc17-c4a259e350e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging to be able to use the new precipitation data\n",
    "R_new_hyd = R_hyd.merge( R_precipitation, left_on = 'date', right_on = 'day')\n",
    "R_new_hyd = R_new_hyd.drop(columns = ['day', 'P [mm]'])\n",
    "R_new_hyd.rename( columns={'P':'P [mm]'}, inplace=True )\n",
    "R_new_hyd.set_index(['date'], drop =True, inplace = True)\n",
    "R_new_hyd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4a39a4-2030-4fcb-a98f-a491c4261f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to define assumptions around dates to choose, based on streamflow and precipitation\n",
    "def tolerance(Q_P_data, date, start, adjust, tolerance_p, P_condition = -999, Q_condition = -999):\n",
    "    \n",
    "    sub_grupp = Q_P_data.copy()\n",
    "    \n",
    "    if adjust == 'start':      \n",
    "        sub_grupp = Q_P_data.loc[start:].copy()       \n",
    "        \n",
    "    elif adjust == 'end': #reverse index to loop backwards\n",
    "        sub_grupp = sub_grupp.loc[:start].copy().iloc[::-1]        \n",
    "    \n",
    "    else:\n",
    "        print('Invalid adjust parameter. Please use \"start\" or \"end\"')\n",
    "        return\n",
    "\n",
    "    \n",
    "    # Reset index if reversed\n",
    "    sub_grupp.reset_index(inplace=True)\n",
    "     \n",
    "    sub_grupp['Q_diff'] = sub_grupp['Q [mm/d]'].diff().fillna(0)\n",
    "\n",
    "    if adjust == 'start':\n",
    "        sub_grupp['tolerance_condition'] = (sub_grupp.Q_diff < tolerance_p * sub_grupp['Q [mm/d]'])\n",
    "\n",
    "    if adjust == 'end':\n",
    "        sub_grupp['tolerance_condition'] = (sub_grupp.Q_diff > -tolerance_p * sub_grupp['Q [mm/d]'])\n",
    "    \n",
    "    if P_condition == -999 == Q_condition:\n",
    "        print('not a valid condition')\n",
    "        return \n",
    "        \n",
    "    elif P_condition == -999:\n",
    "        if Q_condition > 0:\n",
    "            sub_grupp['condition'] = sub_grupp['Q [mm/d]'] > Q_condition\n",
    "        else:\n",
    "            sub_grupp['condition'] = sub_grupp['Q [mm/d]'] < -Q_condition\n",
    "            \n",
    "    elif Q_condition == -999:\n",
    "        if P_condition > 0:\n",
    "            sub_grupp['condition'] = sub_grupp['P [mm]'] > P_condition\n",
    "        else:\n",
    "            sub_grupp['condition'] = sub_grupp['P [mm]'] < -P_condition\n",
    "            \n",
    "    else:\n",
    "        if (Q_condition > 0) & (P_condition > 0):\n",
    "            sub_grupp['condition'] = (sub_grupp['Q [mm/d]'] > Q_condition) & (sub_grupp['P [mm]'] > P_condition)\n",
    "        elif (Q_condition < 0) & (P_condition > 0):\n",
    "            sub_grupp['condition'] = (sub_grupp['Q [mm/d]'] < -Q_condition) & (sub_grupp['P [mm]'] > P_condition)           \n",
    "        elif (Q_condition > 0) & (P_condition < 0):\n",
    "            sub_grupp['condition'] = (sub_grupp['Q [mm/d]'] > Q_condition) & (sub_grupp['P [mm]'] < -P_condition)            \n",
    "        else:\n",
    "            sub_grupp['condition'] = (sub_grupp['Q [mm/d]'] < -Q_condition) & (sub_grupp['P [mm]'] < -P_condition)\n",
    "\n",
    "    #where conditions are met\n",
    "    yesgroup = sub_grupp[(sub_grupp['condition'] == True) & (sub_grupp['tolerance_condition'] == True)] \n",
    "       \n",
    "  \n",
    "    if len(yesgroup) == 0:\n",
    "        print('No data where conditions are met')\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    #where conditions are not met\n",
    "    nogroup = sub_grupp[(sub_grupp['condition'] == False) | (sub_grupp['tolerance_condition'] == False)]  \n",
    "    \n",
    "    if len(nogroup) == 0:\n",
    "        print('nogroup = 0')\n",
    "        return sub_grupp.loc[yesgroup.index[0]:]\n",
    "\n",
    "    if yesgroup.index[0] < nogroup.index[0]:\n",
    "        print('everything is fine')\n",
    "        return sub_grupp.loc[:nogroup.index[0]]\n",
    "    \n",
    "    else:\n",
    "        print('No valid range found between yesgroup and nogroup indices')\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f441fcc5-779d-4dc4-a49f-594adca2fcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assumption of dry dates after the survey date\n",
    "\n",
    "\n",
    "dry_list = []\n",
    "\n",
    "for date in Rdata['Year'].unique():\n",
    "    dry1 = tolerance(R_new_hyd, 'date', date, 'start', 0.05, Q_condition = -999, P_condition = -1)\n",
    "    if len(dry1) == 0:\n",
    "        print('dry1 is empty')\n",
    "        continue\n",
    "    dry1 = dry1[~dry1['date'].isin(Rh['Imagery'])]\n",
    "    dry_imagery = pd.merge(dry1, R_im_date, on = ['date'], how = 'inner')\n",
    "    #print(len(dry_imagery))\n",
    "    dry_points = pd.DataFrame(gdf[gdf['Year']== (date)] .groupby('geometry')['wetdry'].apply(lambda x: (x == 'dry'))).reset_index(drop = False)\n",
    "    dry_points = dry_points.assign(wetdry = 'dry')\n",
    "    dry_im_points = [dry_points.assign(imagery = date) for date in dry_imagery['date']]\n",
    "    #print(len(dry_im_points))\n",
    "    \n",
    "    try:\n",
    "        dry = pd.concat(dry_im_points).drop(columns = ['level_1'])\n",
    "        dry_list.append(dry)\n",
    "    except:\n",
    "        if len(dry_im_points)==0:\n",
    "            print('No data for date '+ date.strftime('%Y-%m-%d'))\n",
    "        else:\n",
    "            dry = dry_im_points[0]\n",
    "            dry_list.append(dry)\n",
    "    #print(len(dry))\n",
    "    \n",
    "\n",
    "dry_df = pd.concat(dry_list)\n",
    "dry_df['assumption'] = len(dry_df)*['assumed dry']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e3b8d-fb1f-4f8a-9a19-629641a8af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gdf['assumption'] = len(gdf)*['survey/imagery match']\n",
    "gdf_imagery = pd.merge(gdf, Rh, left_on = 'Year', right_on = 'Survey', how = 'left')\n",
    "gdf_imagery = gdf_imagery.drop(columns=['Survey', 'sum_P', 'Q_s-i', 'Year'])\n",
    "all_expanded = pd.concat([gdf_imagery, imagery_perennial, dry_df])\n",
    "all_expanded = gpd.GeoDataFrame(all_expanded, geometry = 'geometry', crs='EPSG:26912')\n",
    "all_expanded['x'] = all_expanded.geometry.x\n",
    "all_expanded['y'] = all_expanded.geometry.y\n",
    "all_expanded = all_expanded.rename(columns = {'imagery':'date_first'})\n",
    "all_expanded['date'] = all_expanded['Imagery'].combine_first(all_expanded['date_first'])\n",
    "all_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccf5781-58d0-486d-b9f9-c4dc9b43f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading and concatenating the processed imagery \n",
    "path = '../data/Ramsey/processed_imagery'\n",
    "\n",
    "processed_imagery = glob.glob(path + '/*.csv')\n",
    "processed_imagery.sort(key = lambda x: int(x.split('_buffer_')[1].split('.')[0]))\n",
    "\n",
    "con_ready_imagery = []\n",
    "for processed in processed_imagery:\n",
    "    df= pd.read_csv(processed)\n",
    "    con_ready_imagery.append(df)\n",
    "\n",
    "concatenated = pd.concat(con_ready_imagery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78f8155-fa5f-4ad4-a365-2009862df149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turning the processed imagery into a gdf with x and y columns \n",
    "concatenated['geometry'] = concatenated['geometry'].apply(wkt.loads)\n",
    "gdf_processed = gpd.GeoDataFrame(concatenated, geometry = 'geometry', crs='EPSG:26912')\n",
    "gdf_processed['date'] = pd.to_datetime(gdf_processed['date'], format='%Y%m%d')\n",
    "gdf_processed['x'] = gdf_processed.geometry.x\n",
    "gdf_processed['y'] = gdf_processed.geometry.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd5ce43-9bb8-4927-8561-7a72fd4100da",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_processed = gdf_processed.drop_duplicates()\n",
    "gdf_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fd7be5-0034-444c-8e07-055c97de6337",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = 5\n",
    "all_expanded['x'] = all_expanded['x'].round(precision)\n",
    "all_expanded['y'] = all_expanded['y'].round(precision)\n",
    "gdf_processed['x'] = gdf_processed['x'].round(precision)\n",
    "gdf_processed['y'] = gdf_processed['y'].round(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da7695d-6729-4e3e-a371-6f2d4306df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged = all_expanded.merge(gdf2, on=['date', 'x', 'y'])\n",
    "#merged['geometry'] = merged['geometry'].apply(wkt.loads)\n",
    "#merged = gpd.GeoDataFrame(merged, geometry = 'geometry', crs='EPSG:26912')\n",
    "#merged['x'] = merged.geometry.x\n",
    "#merged['y'] = merged.geometry.y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7300c3-8904-47f4-ac6d-d6dfb5a43963",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = all_expanded.merge(gdf2, on=['date', 'x', 'y'])\n",
    "if len(merged) == 0:\n",
    "    print(\"Merge returned empty. Doing a spatial join based on proximity.\")\n",
    "    # Perform a nearest spatial join\n",
    "    result = gpd.sjoin_nearest(all_expanded, gdf_processed, how='inner', max_distance=0.4)  # Adjust max_distance as needed\n",
    "    result = result[result['date_left'] == result['date_right']]\n",
    "    result = result.drop(columns = ['geometry', 'x_right', 'y_right', 'date_right', 'index_right', 'Imagery', 'date_first'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f317f1e2-aea9-49bb-9696-07fbdf4346e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.rename(columns = {'x_left':'x', 'y_left':'y', 'date_left':'date'})\n",
    "result_sorted = result.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8d9a79-b714-4700-a5d9-19468da7891b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sorted = result_sorted.drop_duplicates()\n",
    "result_sorted.dropna(inplace = True)\n",
    "result_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfc0cec-5b2d-4557-a9c6-fc7fe157417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sorted.to_csv('../data/Ramsey/processed_assumptions/processed_with_dates_and_assumptions.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e661fe5-a3ae-4559-9d67-fb4a436e67a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case geometry gets fixed and the file increases in size as a result \n",
    "#start = 0\n",
    "#splitnum = ?\n",
    "#for i in range(1,splitnum+1):\n",
    "#    newstart = int(len(result_sorted)/splitnum*i)\n",
    "#    result_sorted.iloc[start:newstart].to_csv('../data/Ramsey/processed_assumptions/processed_with_dates_and_assumptions'+str(i)+'.csv',index=False,\n",
    "#                      float_format='%.2f')\n",
    "#    start = newstart\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feec540-ba29-4f5a-a149-f0d6ce1dbbe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
